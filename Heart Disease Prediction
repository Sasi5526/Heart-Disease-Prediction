# -*- coding: utf-8 -*-
"""
Created on Thu May 14 19:31:05 2020

@author: sasim
"""

import pandas as pd
import matplotlib.pyplot as plt
import math
import numpy as np
import seaborn as sb
import plotly.express as px

dataset = pd.read_csv("'''''\\framingham.csv")

dataset.head()
dataset.info()

# checking distributions using histograms
fig = plt.figure(figsize = (15,20))
ax = fig.gca()
dataset.hist(ax = ax)


# checking which features are correlated with each other and are correlated with the outcome variable
dataset_corr = dataset.corr()
sb.heatmap(dataset_corr)

 
#Ploting the relation between gender who have more risk towards Heart disease
plt.figure(figsize=(12,8))
sb.barplot(x=dataset["sex"], y=dataset["TenYearCHD"])
plt.title("Graph showing which gender has more risk of coronary heart disease CHD")
plt.xlabel("0 is female and 1 is male",size=20)
plt.ylabel("total cases", size=20)
plt.xticks(size=12)
plt.yticks(size=12)
#Males have Heavy risk than Females

#Plotting a linegraph to check the relationship between age and cigsPerDay, totChol, glucose.
cig_g = dataset.groupby("age").cigsPerDay.mean()
tot_g = dataset.groupby("age").totChol.mean()
glu_g = dataset.groupby("age").glucose.mean()

plt.figure(figsize=(12,8))
sb.lineplot(data= cig_g, label="cigsPerDay")
sb.lineplot(data= tot_g, label="totChol")
sb.lineplot(data= glu_g, label="glucose")
plt.title("Graph showing totChol and cigsPerDay in every age group.")
plt.xlabel("age", size=20)
plt.ylabel("count", size=20)
plt.xticks(size=12)
plt.yticks(size=12)


# which Age group are more smokers
Smoker = dataset.groupby("age",as_index=False).currentSmoker.sum()

plt.figure(figsize=(12,8))
sb.barplot(x=Smoker["age"], y=Smoker["currentSmoker"])
plt.title("Graph showing which age group has more smokers.")
plt.xlabel("age", size=20)
plt.ylabel("currentSmokers", size=20)
plt.xticks(size=12)
plt.yticks(size=12)
# the people at age 40 are heavy Smokers

#the Risk of Smokers who smoke cigar per day towards heart disease
cig_per = dataset.groupby("TenYearCHD", as_index=False).cigsPerDay.mean()

plt.figure(figsize=(12,8))
sb.barplot(x=cig_per["TenYearCHD"], y=cig_per["cigsPerDay"])
plt.title("Graph showing the relation between cigsPerDay and risk of coronary heart disease.")
plt.xlabel("Rick of CHD", size=20)
plt.ylabel("cigsPerDay", size=20)
plt.xticks(size=12)
plt.yticks(size=12)

###Data Preprocesiing
dataset = dataset.rename({'male': 'sex'}, axis=1)
dataset=dataset.drop("education",axis=1)

# Dropping all rows with missing data
dataset = dataset.dropna()
dataset.isna().sum()
dataset.columns

dataset.info()

###x & y
x= dataset.iloc[:,:-1]
y=dataset.iloc[:,-1]

#Feature Scaling

import statsmodels.api as sm

x_copy = x
reg_OLS = sm.OLS(y,x_copy).fit()
reg_OLS.summary()
max(reg_OLS.pvalues)

x=x.drop(["totChol"],axis=1)

#iteration 1
x_copy = x
reg_OLS = sm.OLS(y,x_copy).fit()
reg_OLS.summary()
max(reg_OLS.pvalues)

x=x.drop(["currentSmoker"],axis=1)

#iteration 2
x_copy = x
reg_OLS = sm.OLS(y,x_copy).fit()
reg_OLS.summary()
max(reg_OLS.pvalues)

x=x.drop(["glucose"],axis=1)

#iteration 3
x_copy = x
reg_OLS = sm.OLS(y,x_copy).fit()
reg_OLS.summary()
max(reg_OLS.pvalues)

reg_OLS.rsquared
reg_OLS.rsquared_adj



# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)



# Fitting Logistic Regression to the Training set
from sklearn.linear_model import LogisticRegression
log = LogisticRegression(penalty='l2',C = 1.0,random_state = 0,
                                solver='sag', multi_class='ovr')
log.fit(x_train, y_train)

# Predicting the Test set results
y_pred_log = log.predict(x_test)
log.predict_proba(x_test)
log.predict_log_proba(x_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score
confusion_matrix(y_test, y_pred_log)
A1=accuracy_score(y_test, y_pred_log)



###Hyperparameter Tuning
#GridSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import make_blobs
from sklearn.model_selection import RepeatedStratifiedKFold

solvers = ['newton-cg', 'lbfgs', 'liblinear']
penalty = ['l2']
c_values = [100, 10, 1.0, 0.1, 0.01]

grid = dict(solver=solvers,penalty=penalty,C=c_values)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
grid_search=GridSearchCV(log,
                         param_grid=grid,
                         n_jobs=-1,
                         cv=cv,
                         scoring='accuracy',
                         error_score=0)

grid_result =grid_search.fit(x_train,y_train)

# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
 
classifier=LogisticRegression(C= 0.01, penalty= 'l2', solver= 'lbfgs')

classifier.fit(x_train,y_train)
y_pred_HT=classifier.predict(x_test)
from sklearn.metrics import accuracy_score
A6 = accuracy_score(y_test,y_pred_HT)


###knearest neighbor
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5,
                        metric='minkowski', p=2)

knn.fit(x_train, y_train)

y_pred_knn = knn.predict(x_test)

from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(y_test, y_pred_knn)
A2=accuracy_score(y_test, y_pred_knn)


###naive bayes

from sklearn.naive_bayes import GaussianNB
NB=GaussianNB()

NB.fit(x_train, y_train)

y_pred_NB = NB.predict(x_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred_NB)
A3=accuracy_score(y_test, y_pred_NB)



####Decision Tree
from sklearn.tree import DecisionTreeClassifier
DT= DecisionTreeClassifier(criterion='entropy')

DT.fit(x_train, y_train)

y_pred_DT = DT.predict(x_test)

from sklearn.metrics import confusion_matrix, accuracy_score
confusion_matrix(y_test, y_pred_DT)
A4=accuracy_score(y_test, y_pred_DT)



###Random Forest
from sklearn.ensemble import RandomForestClassifier


RF = RandomForestClassifier(n_estimators=10,
                                    criterion='entropy')

RF.fit(x_train,y_train)

y_pred_RF = RF.predict(x_test)


from sklearn.metrics import confusion_matrix, accuracy_score
confusion_matrix(y_test, y_pred_RF)
A5 = accuracy_score(y_test, y_pred_RF)


###Hyperparameter Tuning
#GridSearchCV
from sklearn.model_selection import GridSearchCV

parameter=[{'n_estimators':[10,50,100,500,1000],
            'criterion':['gini','entropy'],
            'bootstrap':[True,False],
            'max_depth':[2,3,4,5,6]}]

grid_search=GridSearchCV(RF,
                         param_grid=parameter,
                         scoring='accuracy',
                         cv=10)

grid_search.fit(x_train,y_train)

grid_search.best_score_
grid_search.best_params_
 
classifier=RandomForestClassifier(bootstrap=True,
                                  criterion='gini',
                                  max_depth=6,
                                  n_estimators=10)

classifier.fit(x_train,y_train)
y_pred_HT=classifier.predict(x_test)
from sklearn.metrics import accuracy_score
A7 = accuracy_score(y_test,y_pred_HT)


###Artficial Neural Network
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout



classifier = Sequential()

classifier.add(Dense(units=8,kernel_initializer='uniform',activation='relu',input_dim=11))

classifier.add(Dense(units=8,kernel_initializer='uniform',activation='relu'))

classifier.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))

classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


classifier.fit(x_train,y_train, batch_size=10, epochs=50)

y_pred = classifier.predict(x_test)

y_pred = (y_pred>0.5)

from sklearn.metrics import confusion_matrix, accuracy_score
confusion_matrix(y_test,y_pred)
A8 = accuracy_score(y_test,y_pred)

###ACCURACY
accuracy={'DT ':(A4*100), 'KNN':(A2*100), 'RF':(A5*100), 'Log_reg':(A1*100),'ANN':(A8*100)}
print(accuracy)

#Bar
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.bar(accuracy.keys(),accuracy.values())
plt.show()

#Scatter
plt.scatter(accuracy.keys(),accuracy.values()) 
plt.show()


#ROC AUC curve
from sklearn.metrics import roc_curve, roc_auc_score

results_table = pd.DataFrame(columns = ['models', 'fpr','tpr','auc'])

predictions = {'LR': y_pred_log, 'KNN': y_pred_knn, 'NB': y_pred_NB, 'DT': y_pred_DT,'RF': y_pred_RF}

for key in predictions:
    fpr, tpr, _ = roc_curve(y_test, predictions[key])
    auc = roc_auc_score(y_test, predictions[key])
    
    results_table = results_table.append({'models': key,
                                         'fpr' : fpr,
                                         'tpr' : tpr,
                                         'auc' : auc}, ignore_index=True)
    
results_table.set_index('models', inplace=True)

print(results_table)

fig = plt.figure(figsize = (8,6))

for i in results_table.index:
    plt.plot(results_table.loc[i]['fpr'], 
             results_table.loc[i]['tpr'], 
             label = "{}, AUC={:.3f}".format(i, results_table.loc[i]['auc']))
    
plt.plot([0,1], [0,1], color = 'black', linestyle = '--')

plt.xticks(np.arange(0.0, 1.1, step=0.1))
plt.xlabel("False Positive Rate", fontsize=15)

plt.yticks(np.arange(0.0, 1.1, step=0.1))
plt.ylabel("True Positive Rate", fontsize=15)

plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)
plt.legend(prop = {'size':13}, loc = 'lower right')


plt.show()
